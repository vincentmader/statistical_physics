\paragraph{a) Reservoirs for energy and particle number:
    Consider a system in a fixed volume that not only exchanges energy
    but also particles with the surrounding. To calculate the 
    probability distribution $p_i$, maximize the information 
    (Shannon) entropy $S=-\sum_ip_i\log{p_i}$ under the joint 
    conditions of normalization, of $U=\sum_ip_iE_i$ (mean energy) and 
    $N=\sum_ip_iN_i$ (mean particle number). What physical meaning has 
    the new Lagrangian multiplier associated to the condition in $N$?
} \ \\
\\
    Consider the function 
    \begin{align}
    f(p_i) = - \sum_i p_i \ln p_i - \lambda_1 \left( \sum_i p_i - 1 \right) - \lambda_2 \left( \sum_i p_i E_i - U \right) 
    - \lambda_3 \left( \sum_i p_i N_i - N \right).
    \end{align}
    The Lagrangian multipliers are responsible for the respective constraints like normalization, mean energy and mean particle number.
    The variation with respect to the probability distribution yields
    \begin{align}
    \delta f(p_i) = - \left( \ln p_i + 1 + 
    \lambda_1 + \lambda_2 E_i + \lambda_3 N_i \right) \delta p_i =^{!} 0.
    \end{align}
    Which in fact yields the following for the probability distribution:
    \begin{align}
    p_i = e^{- (1 + \lambda_1 + \lambda_2 E_i + \lambda_3 N_i)}.
    \end{align}
    Now, apply the first constraint (normalization) to find $\lambda_1$:
    \begin{align}
    \sum_i p_i = e^{-(1 + \lambda_1)} Z = 1.
    \end{align}
    Here, we defined the partition sum $Z = \sum_i e^{- (\lambda_2 E_i + \lambda_3 N_i)}$.
    From the other constraints we find $\lambda_2$ and $\lambda_3$. The Lagrangian multiplier for $N$ plays
    the role of the chemical potential. One can find the analogies:
    \begin{align}
    \lambda_2 = \beta \,, \qquad \lambda_3 = \beta \mu \,,
    \end{align}
    to recover the probability distribution
    \begin{align}
    p_i = \frac{1}{Z} e^{-\beta(E_i + \mu N_i)} \,.
    \end{align}


\paragraph{b) Rational probabilities: if one does not know the 
    probabilities of events, one can define so-called 
    rational probabilities $\bar{p}$ such that entropy is maximized 
    subject to the constraints imposed by the available information.
    Assume that in a certain game, a player can score any integer 
    $n=0,1...$ and it is known that the mean score is $\mu$. Use again
    the entropy and the method of Lagrange multipliers to show that 
    when imposing the relevant constraints the rational choice is 
    $\bar{p}_n=\frac{\mu^n}{(1+\mu)^{n+1}}$
} \ \\
\\
    We follow the same procedure. Now, the function would look like this:
    \begin{align}
    f(p_i)=-\sum_i p_i\ln p_i - \lambda_1 \left( \sum_i p_i - 1 \right) - \lambda_2 \left(\sum_n p_nn-\mu\right) \,.
    \end{align}
    Variation with respect to $p_n$ yields
    \begin{align}
    p_n = e^{- (1 + \lambda_1 + \lambda_2 n)} \,.
    \end{align}
    From the normalization condition we get:
    \begin{align}
    p_n = \frac{1}{Z} e^{- \lambda_2 n} \,,
    \end{align}
    where $Z = \sum_n e^{- \lambda_2 n} = \frac{1}{1-e^{- \lambda_2}}$ is the geometrical series. From the constraint for $\mu$ we find:
    \begin{align}
        \mu = \sum_n n p_n = \frac{1}{Z} \sum_n n e^{-\lambda_2 n} = - \frac{1}{Z} \partial_{\lambda_2} \sum_n e^{-\lambda_2 n} = - \partial_{\lambda_2} \ln Z = \frac{e^{-\lambda_2}}{1-e^{-\lambda_2}} \,.
    \end{align}
    This can be inverted to give
    \begin{align}
    \lambda_2 = \ln\left(\frac{1+\mu}{\mu}\right) \,.
    \end{align}
    Thus, we obtain
    \begin{align}
    p_n = \frac{1}{Z} e^{- \lambda_2 n} = e^{- \lambda_2 n} \left(1-e^{-\lambda_2}\right) = \left(\frac{\mu}{1+\mu}\right)^n \left(1-\frac{\mu}{1+\mu}\right) = \frac{\mu^n}{(1+\mu)^{n+1}} \,.
    \end{align}
